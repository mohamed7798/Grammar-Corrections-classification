{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b74c2902",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "def add_noise(sentence, p=0.2):\n",
    "    words = sentence.split()\n",
    "    noisy_words = []\n",
    "    for word in words:\n",
    "        if random.random() < p:\n",
    "            # Add noise to the word\n",
    "            noisy_word = ''.join(random.sample(word, len(word)))\n",
    "            noisy_words.append(noisy_word)\n",
    "        else:\n",
    "            noisy_words.append(word)\n",
    "    return ' '.join(noisy_words)\n",
    "\n",
    "# Load the training dataset\n",
    "train_df = pd.read_csv(r'D:\\sell\\archive (1)\\train_updated.csv')\n",
    "eval_df = pd.read_csv(r'D:\\sell\\archive (1)\\eval_updated.csv')\n",
    "train_df = pd.concat([train_df, eval_df], ignore_index=True)\n",
    "\n",
    "\n",
    "# Augment the data by adding noise to each sentence\n",
    "augmented_sentences = [add_noise(sentence) for sentence in train_df['input'].values]\n",
    "\n",
    "# Combine the original and augmented data\n",
    "all_sentences = list(train_df['input'].values) + augmented_sentences\n",
    "all_targets = list(train_df['target'].values) + list(train_df['target'].values)\n",
    "\n",
    "# Create a new dataframe with the combined data\n",
    "new_train_df = pd.DataFrame({'input': all_sentences, 'target': all_targets})\n",
    "\n",
    "# Save the new dataframe to a CSV file\n",
    "new_train_df.to_csv(r'D:\\sell\\archive (1)\\augmented_train_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "712c17f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>So I think we can not live if old people coul...</td>\n",
       "      <td>So I think we would not be alive if our ancest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>So I think we can not live if old people coul...</td>\n",
       "      <td>So I think we could not live if older people d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>So I think we can not live if old people coul...</td>\n",
       "      <td>So I think we can not live if old people could...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>So I think we can not live if old people coul...</td>\n",
       "      <td>So I think we can not live if old people can n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>For not use car .</td>\n",
       "      <td>Not for use with a car .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12003</th>\n",
       "      <td>In toreh words , the image in the TV comercial...</td>\n",
       "      <td>In other words , the image in TV commercials i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12004</th>\n",
       "      <td>Members gather yomen for the funeral nad lpeh ...</td>\n",
       "      <td>Members gather money for the funeral to help t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12005</th>\n",
       "      <td>Members gather money for the funeral nda help ...</td>\n",
       "      <td>Members gather money for the funeral to help t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12006</th>\n",
       "      <td>eMbsmer reaght oenmy for the funeral and leph ...</td>\n",
       "      <td>Members gather money for the funeral and help ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12007</th>\n",
       "      <td>Members gather money for eht funeral and eplh ...</td>\n",
       "      <td>Members gather money for the funeral and help ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12008 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   input  \\\n",
       "0       So I think we can not live if old people coul...   \n",
       "1       So I think we can not live if old people coul...   \n",
       "2       So I think we can not live if old people coul...   \n",
       "3       So I think we can not live if old people coul...   \n",
       "4                                     For not use car .    \n",
       "...                                                  ...   \n",
       "12003  In toreh words , the image in the TV comercial...   \n",
       "12004  Members gather yomen for the funeral nad lpeh ...   \n",
       "12005  Members gather money for the funeral nda help ...   \n",
       "12006  eMbsmer reaght oenmy for the funeral and leph ...   \n",
       "12007  Members gather money for eht funeral and eplh ...   \n",
       "\n",
       "                                                  target  \n",
       "0      So I think we would not be alive if our ancest...  \n",
       "1      So I think we could not live if older people d...  \n",
       "2      So I think we can not live if old people could...  \n",
       "3      So I think we can not live if old people can n...  \n",
       "4                              Not for use with a car .   \n",
       "...                                                  ...  \n",
       "12003  In other words , the image in TV commercials i...  \n",
       "12004  Members gather money for the funeral to help t...  \n",
       "12005  Members gather money for the funeral to help t...  \n",
       "12006  Members gather money for the funeral and help ...  \n",
       "12007  Members gather money for the funeral and help ...  \n",
       "\n",
       "[12008 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d13ac95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>So I think we can not live if old people coul...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>So I think we can not live if old people coul...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>So I think we can not live if old people coul...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>So I think we can not live if old people coul...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>For not use car .</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32001</th>\n",
       "      <td>More than a hundred thousand people are suppor...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32002</th>\n",
       "      <td>The reason why we chose this subject a few day...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32003</th>\n",
       "      <td>When you buy CBS is Criminal Minds DVD in Japa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32004</th>\n",
       "      <td>but it is dizzying for me T T</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32005</th>\n",
       "      <td>in several times .</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32006 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   input  target\n",
       "0       So I think we can not live if old people coul...       0\n",
       "1       So I think we can not live if old people coul...       0\n",
       "2       So I think we can not live if old people coul...       0\n",
       "3       So I think we can not live if old people coul...       0\n",
       "4                                     For not use car .        0\n",
       "...                                                  ...     ...\n",
       "32001  More than a hundred thousand people are suppor...       1\n",
       "32002  The reason why we chose this subject a few day...       1\n",
       "32003  When you buy CBS is Criminal Minds DVD in Japa...       1\n",
       "32004                      but it is dizzying for me T T       1\n",
       "32005                                 in several times .       0\n",
       "\n",
       "[32006 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "lb = LabelBinarizer()\n",
    "\n",
    "# fit and transform the target column using the LabelBinarizer\n",
    "new_train_df['target'] = lb.fit_transform(new_train_df['target'])\n",
    "another_data = pd.read_csv(r'D:\\sell\\archive (1)\\NLP Assignment\\train_data.csv')\n",
    "another_data = another_data.rename(columns={'input': 'input', 'labels': 'target'})\n",
    "df = pd.concat([new_train_df, another_data], ignore_index=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6eb44f46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    22005\n",
       "1    10001\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb92aad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 50, 128)           2718848   \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 128)               131584    \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " leaky_re_lu_2 (LeakyReLU)   (None, 64)                0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,860,801\n",
      "Trainable params: 2,860,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "361/361 [==============================] - ETA: 0s - loss: 0.5905 - accuracy: 0.6864\n",
      "Epoch 1: val_accuracy improved from -inf to 0.68478, saving model to best_model.h5\n",
      "361/361 [==============================] - 79s 201ms/step - loss: 0.5905 - accuracy: 0.6864 - val_loss: 0.5378 - val_accuracy: 0.6848\n",
      "Epoch 2/100\n",
      "361/361 [==============================] - ETA: 0s - loss: 0.4867 - accuracy: 0.6975\n",
      "Epoch 2: val_accuracy did not improve from 0.68478\n",
      "361/361 [==============================] - 71s 197ms/step - loss: 0.4867 - accuracy: 0.6975 - val_loss: 0.5313 - val_accuracy: 0.6574\n",
      "Epoch 3/100\n",
      "361/361 [==============================] - ETA: 0s - loss: 0.4566 - accuracy: 0.7150\n",
      "Epoch 3: val_accuracy did not improve from 0.68478\n",
      "361/361 [==============================] - 71s 198ms/step - loss: 0.4566 - accuracy: 0.7150 - val_loss: 0.5799 - val_accuracy: 0.6440\n",
      "Epoch 4/100\n",
      "361/361 [==============================] - ETA: 0s - loss: 0.4374 - accuracy: 0.7236\n",
      "Epoch 4: val_accuracy did not improve from 0.68478\n",
      "361/361 [==============================] - 70s 194ms/step - loss: 0.4374 - accuracy: 0.7236 - val_loss: 0.5821 - val_accuracy: 0.6376\n",
      "Epoch 5/100\n",
      "361/361 [==============================] - ETA: 0s - loss: 0.4236 - accuracy: 0.7354\n",
      "Epoch 5: val_accuracy did not improve from 0.68478\n",
      "361/361 [==============================] - 71s 196ms/step - loss: 0.4236 - accuracy: 0.7354 - val_loss: 0.5957 - val_accuracy: 0.6318\n",
      "Epoch 6/100\n",
      "361/361 [==============================] - ETA: 0s - loss: 0.4136 - accuracy: 0.7456\n",
      "Epoch 6: val_accuracy did not improve from 0.68478\n",
      "361/361 [==============================] - 71s 196ms/step - loss: 0.4136 - accuracy: 0.7456 - val_loss: 0.6112 - val_accuracy: 0.6294\n",
      "Epoch 7/100\n",
      "361/361 [==============================] - ETA: 0s - loss: 0.4036 - accuracy: 0.7521\n",
      "Epoch 7: val_accuracy did not improve from 0.68478\n",
      "361/361 [==============================] - 71s 197ms/step - loss: 0.4036 - accuracy: 0.7521 - val_loss: 0.6975 - val_accuracy: 0.6311\n",
      "Epoch 8/100\n",
      "361/361 [==============================] - ETA: 0s - loss: 0.3951 - accuracy: 0.7611\n",
      "Epoch 8: val_accuracy did not improve from 0.68478\n",
      "361/361 [==============================] - 71s 197ms/step - loss: 0.3951 - accuracy: 0.7611 - val_loss: 0.7579 - val_accuracy: 0.6249\n",
      "Epoch 9/100\n",
      "361/361 [==============================] - ETA: 0s - loss: 0.3850 - accuracy: 0.7715\n",
      "Epoch 9: val_accuracy did not improve from 0.68478\n",
      "361/361 [==============================] - 71s 195ms/step - loss: 0.3850 - accuracy: 0.7715 - val_loss: 0.7834 - val_accuracy: 0.6273\n",
      "Epoch 10/100\n",
      "361/361 [==============================] - ETA: 0s - loss: 0.3745 - accuracy: 0.7798\n",
      "Epoch 10: val_accuracy did not improve from 0.68478\n",
      "361/361 [==============================] - 71s 196ms/step - loss: 0.3745 - accuracy: 0.7798 - val_loss: 0.7582 - val_accuracy: 0.6185\n",
      "Epoch 11/100\n",
      "361/361 [==============================] - ETA: 0s - loss: 0.3651 - accuracy: 0.7857\n",
      "Epoch 11: val_accuracy did not improve from 0.68478\n",
      "361/361 [==============================] - 70s 194ms/step - loss: 0.3651 - accuracy: 0.7857 - val_loss: 0.8692 - val_accuracy: 0.6209\n",
      "Epoch 12/100\n",
      "361/361 [==============================] - ETA: 0s - loss: 0.3559 - accuracy: 0.7903\n",
      "Epoch 12: val_accuracy did not improve from 0.68478\n",
      "361/361 [==============================] - 71s 197ms/step - loss: 0.3559 - accuracy: 0.7903 - val_loss: 0.8619 - val_accuracy: 0.6181\n",
      "Epoch 13/100\n",
      "361/361 [==============================] - ETA: 0s - loss: 0.3487 - accuracy: 0.7970\n",
      "Epoch 13: val_accuracy did not improve from 0.68478\n",
      "361/361 [==============================] - 71s 197ms/step - loss: 0.3487 - accuracy: 0.7970 - val_loss: 0.9183 - val_accuracy: 0.6181\n",
      "Epoch 14/100\n",
      "361/361 [==============================] - ETA: 0s - loss: 0.3406 - accuracy: 0.8008\n",
      "Epoch 14: val_accuracy did not improve from 0.68478\n",
      "361/361 [==============================] - 71s 196ms/step - loss: 0.3406 - accuracy: 0.8008 - val_loss: 0.9063 - val_accuracy: 0.6108\n",
      "Epoch 15/100\n",
      "361/361 [==============================] - ETA: 0s - loss: 0.3312 - accuracy: 0.8070\n",
      "Epoch 15: val_accuracy did not improve from 0.68478\n",
      "361/361 [==============================] - 71s 196ms/step - loss: 0.3312 - accuracy: 0.8070 - val_loss: 1.0785 - val_accuracy: 0.6190\n",
      "Epoch 16/100\n",
      "361/361 [==============================] - ETA: 0s - loss: 0.3254 - accuracy: 0.8116\n",
      "Epoch 16: val_accuracy did not improve from 0.68478\n",
      "361/361 [==============================] - 71s 196ms/step - loss: 0.3254 - accuracy: 0.8116 - val_loss: 1.0236 - val_accuracy: 0.6126\n",
      "Epoch 17/100\n",
      "361/361 [==============================] - ETA: 0s - loss: 0.3177 - accuracy: 0.8152\n",
      "Epoch 17: val_accuracy did not improve from 0.68478\n",
      "361/361 [==============================] - 71s 197ms/step - loss: 0.3177 - accuracy: 0.8152 - val_loss: 1.1570 - val_accuracy: 0.6105\n",
      "Epoch 17: early stopping\n",
      "{'loss': [0.5904825329780579, 0.4867287278175354, 0.45660191774368286, 0.4373728632926941, 0.4235549569129944, 0.41356465220451355, 0.4036392867565155, 0.3950687348842621, 0.3849811255931854, 0.3745124042034149, 0.36509591341018677, 0.3558848202228546, 0.34866803884506226, 0.3406256437301636, 0.3312496542930603, 0.3253827393054962, 0.3177197277545929], 'accuracy': [0.6864259839057922, 0.6975351572036743, 0.7149800658226013, 0.7236157059669495, 0.7353758215904236, 0.7456170916557312, 0.7520829439163208, 0.7610657811164856, 0.7715240120887756, 0.7798125147819519, 0.7856708765029907, 0.7903141975402832, 0.7969536781311035, 0.8007724285125732, 0.8069779276847839, 0.8116212487220764, 0.8152230381965637], 'val_loss': [0.5378082990646362, 0.5312617421150208, 0.5798742175102234, 0.5820626020431519, 0.5957164168357849, 0.6111960411071777, 0.6974838972091675, 0.7579103112220764, 0.7833743095397949, 0.7581539750099182, 0.8691878914833069, 0.8618537187576294, 0.9183415770530701, 0.9062564373016357, 1.0785404443740845, 1.0236173868179321, 1.1569887399673462], 'val_accuracy': [0.6847769618034363, 0.6573511362075806, 0.643985390663147, 0.6375629305839539, 0.6318347454071045, 0.629404604434967, 0.631140410900116, 0.6248915195465088, 0.6273216605186462, 0.6184689998626709, 0.6208991408348083, 0.618121862411499, 0.618121862411499, 0.6108314394950867, 0.6189897656440735, 0.6125672459602356, 0.6104843020439148]}\n",
      "101/101 [==============================] - 2s 16ms/step - loss: 0.5259 - accuracy: 0.6935\n",
      "Test score: 0.5259342193603516\n",
      "Test accuracy: 0.6935332417488098\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, LeakyReLU\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras import regularizers, optimizers\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "\n",
    "# Perform data preprocessing\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df['input'] = df['input'].apply(lambda x: x.lower())\n",
    "df['input'] = df['input'].apply(lambda x: re.sub('[^a-zA-z0-9\\s]', '', x))\n",
    "df['input'] = df['input'].apply(lambda x: re.sub('\\n', ' ', x))\n",
    "df['input'] = df['input'].apply(lambda x: re.sub('\\s+', ' ', x.strip()))\n",
    "df['input'] = df['input'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n",
    "\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(df['input'].values)\n",
    "X = tokenizer.texts_to_sequences(df['input'].values)\n",
    "X = pad_sequences(X, maxlen=50)\n",
    "\n",
    "# Split data into train and test sets\n",
    "Y = df['target'].values\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.1, random_state=42)\n",
    "\n",
    "# Define the model architecture\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=128, input_length=X.shape[1]))\n",
    "model.add(LSTM(units=128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(units=64, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
    "model.add(LeakyReLU(alpha = 0.1))\n",
    "model.add(Dense(units=32, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
    "\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "np.random.seed(42)\n",
    "optimizer = optimizers.Adam(learning_rate = 0.001)\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "# Set early stopping and model checkpoint callbacks\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=15)\n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, Y_train, epochs=100, batch_size=64, validation_split=0.2, callbacks=[es, mc])\n",
    "\n",
    "# Print the training loss and accuracy for each epoch\n",
    "print(history.history)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "model.load_weights('best_model.h5')\n",
    "score, acc = model.evaluate(X_test, Y_test, verbose=1)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b0f0224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_5 (Embedding)     (None, 50, 128)           2718848   \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 256)              263168    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 64)                16448     \n",
      "                                                                 \n",
      " leaky_re_lu_4 (LeakyReLU)   (None, 64)                0         \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,000,577\n",
      "Trainable params: 3,000,577\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "361/361 [==============================] - ETA: 0s - loss: 0.5866 - accuracy: 0.6876\n",
      "Epoch 1: val_accuracy improved from -inf to 0.68478, saving model to best_model.h5\n",
      "361/361 [==============================] - 394s 1s/step - loss: 0.5866 - accuracy: 0.6876 - val_loss: 0.5441 - val_accuracy: 0.6848\n",
      "Epoch 2/100\n",
      "361/361 [==============================] - ETA: 0s - loss: 0.4873 - accuracy: 0.6957\n",
      "Epoch 2: val_accuracy did not improve from 0.68478\n",
      "361/361 [==============================] - 369s 1s/step - loss: 0.4873 - accuracy: 0.6957 - val_loss: 0.5496 - val_accuracy: 0.6487\n",
      "Epoch 3/100\n",
      "361/361 [==============================] - ETA: 0s - loss: 0.4565 - accuracy: 0.7144\n",
      "Epoch 3: val_accuracy did not improve from 0.68478\n",
      "361/361 [==============================] - 371s 1s/step - loss: 0.4565 - accuracy: 0.7144 - val_loss: 0.5645 - val_accuracy: 0.6403\n",
      "Epoch 4/100\n",
      "361/361 [==============================] - ETA: 0s - loss: 0.4371 - accuracy: 0.7244\n",
      "Epoch 4: val_accuracy did not improve from 0.68478\n",
      "361/361 [==============================] - 373s 1s/step - loss: 0.4371 - accuracy: 0.7244 - val_loss: 0.6073 - val_accuracy: 0.6416\n",
      "Epoch 5/100\n",
      "361/361 [==============================] - ETA: 0s - loss: 0.4230 - accuracy: 0.7351\n",
      "Epoch 5: val_accuracy did not improve from 0.68478\n",
      "361/361 [==============================] - 377s 1s/step - loss: 0.4230 - accuracy: 0.7351 - val_loss: 0.6516 - val_accuracy: 0.6292\n",
      "Epoch 6/100\n",
      "361/361 [==============================] - ETA: 0s - loss: 0.4142 - accuracy: 0.7445\n",
      "Epoch 6: val_accuracy did not improve from 0.68478\n",
      "361/361 [==============================] - 377s 1s/step - loss: 0.4142 - accuracy: 0.7445 - val_loss: 0.7290 - val_accuracy: 0.6306\n",
      "Epoch 7/100\n",
      "361/361 [==============================] - ETA: 0s - loss: 0.4080 - accuracy: 0.7488\n",
      "Epoch 7: val_accuracy did not improve from 0.68478\n",
      "361/361 [==============================] - 385s 1s/step - loss: 0.4080 - accuracy: 0.7488 - val_loss: 0.6384 - val_accuracy: 0.6285\n",
      "Epoch 8/100\n",
      "361/361 [==============================] - ETA: 0s - loss: 0.4019 - accuracy: 0.7534\n",
      "Epoch 8: val_accuracy did not improve from 0.68478\n",
      "361/361 [==============================] - 385s 1s/step - loss: 0.4019 - accuracy: 0.7534 - val_loss: 0.6941 - val_accuracy: 0.6294\n",
      "Epoch 9/100\n",
      "361/361 [==============================] - ETA: 0s - loss: 0.3942 - accuracy: 0.7587\n",
      "Epoch 9: val_accuracy did not improve from 0.68478\n",
      "361/361 [==============================] - 379s 1s/step - loss: 0.3942 - accuracy: 0.7587 - val_loss: 0.8012 - val_accuracy: 0.6322\n",
      "Epoch 10/100\n",
      "361/361 [==============================] - ETA: 0s - loss: 0.3866 - accuracy: 0.7615\n",
      "Epoch 10: val_accuracy did not improve from 0.68478\n",
      "361/361 [==============================] - 375s 1s/step - loss: 0.3866 - accuracy: 0.7615 - val_loss: 0.7610 - val_accuracy: 0.6325\n",
      "Epoch 11/100\n",
      "361/361 [==============================] - ETA: 0s - loss: 0.3785 - accuracy: 0.7696\n",
      "Epoch 11: val_accuracy did not improve from 0.68478\n",
      "361/361 [==============================] - 380s 1s/step - loss: 0.3785 - accuracy: 0.7696 - val_loss: 0.7742 - val_accuracy: 0.6280\n",
      "Epoch 12/100\n",
      "361/361 [==============================] - ETA: 0s - loss: 0.3710 - accuracy: 0.7756\n",
      "Epoch 12: val_accuracy did not improve from 0.68478\n",
      "361/361 [==============================] - 368s 1s/step - loss: 0.3710 - accuracy: 0.7756 - val_loss: 0.8152 - val_accuracy: 0.6303\n",
      "Epoch 13/100\n",
      "361/361 [==============================] - ETA: 0s - loss: 0.3627 - accuracy: 0.7815\n",
      "Epoch 13: val_accuracy did not improve from 0.68478\n",
      "361/361 [==============================] - 383s 1s/step - loss: 0.3627 - accuracy: 0.7815 - val_loss: 0.8900 - val_accuracy: 0.6261\n",
      "Epoch 14/100\n",
      "361/361 [==============================] - ETA: 0s - loss: 0.3565 - accuracy: 0.7872\n",
      "Epoch 14: val_accuracy did not improve from 0.68478\n",
      "361/361 [==============================] - 382s 1s/step - loss: 0.3565 - accuracy: 0.7872 - val_loss: 0.8146 - val_accuracy: 0.6268\n",
      "Epoch 15/100\n",
      "361/361 [==============================] - ETA: 0s - loss: 0.3489 - accuracy: 0.7913\n",
      "Epoch 15: val_accuracy did not improve from 0.68478\n",
      "361/361 [==============================] - 379s 1s/step - loss: 0.3489 - accuracy: 0.7913 - val_loss: 0.9766 - val_accuracy: 0.6273\n",
      "Epoch 16/100\n",
      "361/361 [==============================] - ETA: 0s - loss: 0.3395 - accuracy: 0.7974\n",
      "Epoch 16: val_accuracy did not improve from 0.68478\n",
      "361/361 [==============================] - 376s 1s/step - loss: 0.3395 - accuracy: 0.7974 - val_loss: 0.9473 - val_accuracy: 0.6282\n",
      "Epoch 16: early stopping\n",
      "{'loss': [0.5866082310676575, 0.48734617233276367, 0.45650723576545715, 0.43710699677467346, 0.4230131506919861, 0.41423746943473816, 0.407999187707901, 0.40188971161842346, 0.39422300457954407, 0.3866218030452728, 0.3784799575805664, 0.3710009753704071, 0.36266764998435974, 0.3565383851528168, 0.34890931844711304, 0.33952656388282776], 'accuracy': [0.6875542402267456, 0.6957125663757324, 0.7143725156784058, 0.7243534326553345, 0.7351154088973999, 0.7445322275161743, 0.7487849593162537, 0.7534282207489014, 0.7586790323257446, 0.7614997625350952, 0.7696146368980408, 0.7756465673446655, 0.7815049290657043, 0.7871897220611572, 0.7912688851356506, 0.7974309921264648], 'val_loss': [0.5441092848777771, 0.5496298670768738, 0.5644865036010742, 0.6072580814361572, 0.6515597105026245, 0.7289571762084961, 0.6383896470069885, 0.6940566897392273, 0.8012470602989197, 0.7610029578208923, 0.7741872668266296, 0.815173864364624, 0.8899533748626709, 0.8145791292190552, 0.9766275882720947, 0.9472838640213013], 'val_accuracy': [0.6847769618034363, 0.6486721038818359, 0.6403402090072632, 0.6415553092956543, 0.6292310357093811, 0.6306197047233582, 0.6285367012023926, 0.629404604434967, 0.6321819424629211, 0.632529079914093, 0.6280159950256348, 0.6302725076675415, 0.6261065602302551, 0.6268008947372437, 0.6273216605186462, 0.6281895637512207]}\n",
      "101/101 [==============================] - 6s 57ms/step - loss: 0.5267 - accuracy: 0.6935\n",
      "Test score: 0.5266979932785034\n",
      "Test accuracy: 0.6935332417488098\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
    "# Define the model architecture\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=128, input_length=X.shape[1]))\n",
    "model.add(Bidirectional(LSTM(units=128, dropout=0.2, recurrent_dropout=0.2)))\n",
    "model.add(Dense(units=64, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Dense(units=32, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "np.random.seed(42)\n",
    "optimizer = optimizers.Adam(learning_rate = 0.001)\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "# Set early stopping and model checkpoint callbacks\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=15)\n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, Y_train, epochs=100, batch_size=64, validation_split=0.2, callbacks=[es, mc])\n",
    "\n",
    "# Print the training loss and accuracy for each epoch\n",
    "print(history.history)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "model.load_weights('best_model.h5')\n",
    "score, acc = model.evaluate(X_test, Y_test, verbose=1)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979f5734",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6313ad68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
